{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptzlbG3S92_t"
      },
      "source": [
        "# Predicting Movie Revenue from Pre‚ÄëRelease Features (The Movies Dataset / Kaggle)\n",
        "\n",
        "## Introduction\n",
        "This Jupyter notebook is part of the final project for the **Machine Learning** course at **GISMA University of Applied Sciences**.  \n",
        "The goal is to predict a movie‚Äôs **box‚Äëoffice revenue** using only **pre‚Äërelease information** (features available before the movie is released), such as budget, runtime, genres, release year, and historical ‚Äútrack record‚Äù features for cast and key crew roles.\n",
        "\n",
        "The dataset used in this project is publicly available on Kaggle: **‚ÄúThe Movies Dataset‚Äù (Rounak Banik)**. The metadata includes movie attributes (e.g., budget, runtime, genres, release date) plus cast/crew lists.\n",
        "\n",
        "Predicting revenue is challenging because revenues are **highly skewed** and influenced by many interacting factors. To handle skewness, I model **log1p(revenue)** and later convert predictions back to the revenue scale.  \n",
        "Finally, I interpret the model output as a **revenue interval** (a practical range) rather than a single point estimate, for example, this model does not say that the revenue will be exactly 10.5 million dollars. Instead, it might say that the expected revenue is around 10 million dollars with an uncertainty of about ¬±3 million, which means the value is likely to be between roughly 7 and 13 million dollars.\n",
        "\n",
        "I will explain all details and decisions and discuss about results and what can be improve in next versions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKp0LjKU92_v"
      },
      "source": [
        "## Abstract (What this notebook delivers)\n",
        "- **Task:** Supervised regression to predict **log1p(revenue)**.\n",
        "- **Input features:** Only **pre‚Äërelease** attributes + **time‚Äëaware historical aggregates** for top cast and key crew roles.\n",
        "- **Train/Test split:** **Temporal split** to mimic real forecasting (train: release_year < 2015, test: ‚â• 2015).\n",
        "- **Model selection:** Compare multiple regressors using cross‚Äëvalidation on the **train** period only.\n",
        "- **Final model:** `HistGradientBoostingRegressor` (best overall on the held‚Äëout test set in this project).\n",
        "- **Final test performance (log scale):** MAE ‚âà **0.26** (typical multiplicative error about **√ó/√∑ 1.30**).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMhqwNy7eOKT"
      },
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3J5v-2FvLFZa"
      },
      "outputs": [],
      "source": [
        "!pip install -q kagglehub\n",
        "!pip install -q pandas\n",
        "!pip install -q numpy\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q matplotlib\n",
        "!pip install -q seaborn\n",
        "!pip install -q tqdm joblib tqdm-joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TLUWiJFecIA"
      },
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKMaXDLweenr"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "import os\n",
        "import ast\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from joblib import parallel_backend\n",
        "from tqdm_joblib import tqdm_joblib\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl83PKzBe4SX"
      },
      "source": [
        "## 3. Data Preprocessing:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IXJrAOafF9n"
      },
      "source": [
        "### 3.1. File(Data) Loading\n",
        "\n",
        "The dataset is loaded programmatically from Kaggle using Kaggle Hub to ensure full reproducibility without requiring authentication or local file dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL-GBPfkiLix"
      },
      "outputs": [],
      "source": [
        "dataset_path = kagglehub.dataset_download(\"rounakbanik/the-movies-dataset\")\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Dataset Filename:\", os.listdir(dataset_path)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBswSdi0e9nP"
      },
      "source": [
        "### 3.2 Load Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s7v8-sQmOoQ"
      },
      "outputs": [],
      "source": [
        "movies_path  = os.path.join(dataset_path, \"movies_metadata.csv\")\n",
        "credits_path = os.path.join(dataset_path, \"credits.csv\")\n",
        "\n",
        "movies = pd.read_csv(movies_path, low_memory=False)\n",
        "credits = pd.read_csv(credits_path)\n",
        "\n",
        "print(\"movies:\", movies.shape)\n",
        "print(\"credits:\", credits.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoYY1Qprmi3z"
      },
      "source": [
        "### 3.3. Clean and normalize the join key (id)\n",
        "\n",
        "The \"movies_metadata.csv\" sometimes contains non-numeric IDs, so I convert safely.\n",
        "I did that and in this moment that I checked and tried to normalize data 3 movies out of 45466 reduced from the main list but I'm keeping it anyway because it's a good approach to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYxnpWHPmmKd"
      },
      "outputs": [],
      "source": [
        "movies[\"id\"] = pd.to_numeric(movies[\"id\"], errors=\"coerce\")\n",
        "credits[\"id\"] = pd.to_numeric(credits[\"id\"], errors=\"coerce\")\n",
        "\n",
        "movies = movies.dropna(subset=[\"id\"]).copy()\n",
        "credits = credits.dropna(subset=[\"id\"]).copy()\n",
        "\n",
        "movies[\"id\"] = movies[\"id\"].astype(int)\n",
        "credits[\"id\"] = credits[\"id\"].astype(int)\n",
        "\n",
        "print(\"movies after id cleaning:\", movies.shape)\n",
        "print(\"credits after id cleaning:\", credits.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ3RMvdCniQ8"
      },
      "source": [
        "### 3.4. Check for duplicates and remove them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB07FypxniCX"
      },
      "outputs": [],
      "source": [
        "print(\"Duplicate ids in movies:\", movies[\"id\"].duplicated().sum())\n",
        "print(\"Duplicate ids in credits:\", credits[\"id\"].duplicated().sum())\n",
        "credits = credits.drop_duplicates(subset=[\"id\"]).copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUXUF9wYnv1X"
      },
      "source": [
        "### 3.5. Merging credits with movie metadata\n",
        "\n",
        "I chose left join on these files because it keeps all movies from metadata, adds cast/crew where available to have a unified table with all the features that I think are useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UHCQw_Nnvrc"
      },
      "outputs": [],
      "source": [
        "df = movies.merge(credits, on=\"id\", how=\"left\")\n",
        "\n",
        "print(\"Merged df:\", df.shape)\n",
        "df.head(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPyDZaZPyXll"
      },
      "source": [
        "### 3.6. Data profiling and initial cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkmiSOz6yXUL"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset shape:\", df.shape)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6.0 A major issue in the dataset\n",
        "\n",
        "The raw dataset has a serious limitation: **around 84% of movies have a recorded revenue of zero**. In many cases, this does not mean that the movie truly earned zero, but rather that the revenue is missing or not reported. This makes the raw revenue column unreliable for a large part of the dataset.\n",
        "\n",
        "In the rest of the project, I continue working with the data but focus on the subset of movies with **valid (non-zero) revenue**, and apply cleaning and feature extraction on this reduced sample to train the model. This issue and its impact on the results are discussed later in Section 9 (Discussion and limitations).\n"
      ],
      "metadata": {
        "id": "Ijeb244Dy7c4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "total_rows = len(df)\n",
        "zero_revenue = (df[\"revenue\"] == 0).sum()\n",
        "pos_revenue = (df[\"revenue\"] > 0).sum()\n",
        "\n",
        "print(f\"Total movies:          {total_rows}\")\n",
        "print(f\"Revenue > 0:           {pos_revenue} ({pos_revenue / total_rows:.2%})\")\n",
        "print(f\"Revenue == 0:          {zero_revenue} ({zero_revenue / total_rows:.2%})\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GmEhE1wvxu8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW_0_0_ayq7S"
      },
      "source": [
        "### 3.6.1 Missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLXf94-Kyqyr"
      },
      "outputs": [],
      "source": [
        "missing = df.isnull().mean().sort_values(ascending=False)\n",
        "missing[missing > 0].head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joqdK0ND92_1"
      },
      "source": [
        "### 3.6.2 Dropping irrelevant and leakage‚Äëprone columns (before feature engineering)\n",
        "\n",
        "Before building features, I remove columns that are either:\n",
        "- **Irrelevant** for pre‚Äërelease forecasting (e.g., poster paths, free‚Äëtext tagline), or\n",
        "- **Leakage / post‚Äërelease proxies**, such as vote statistics that strongly depend on audience reactions after release.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbsJobJdzNqT"
      },
      "outputs": [],
      "source": [
        "IRRELEVANT_COLS = [\n",
        "    \"homepage\",\n",
        "    \"belongs_to_collection\",\n",
        "    \"overview\",\n",
        "    \"poster_path\",\n",
        "    \"imdb_id\",\n",
        "    \"original_title\",\n",
        "    \"video\",\n",
        "    \"tagline\",\n",
        "]\n",
        "\n",
        "LEAKING_COLS = [\n",
        "    \"popularity\",\n",
        "    \"vote_average\",\n",
        "    \"vote_count\",\n",
        "]\n",
        "\n",
        "CLEANING_DROP_COLS = IRRELEVANT_COLS + LEAKING_COLS\n",
        "\n",
        "df = df.drop(columns=[c for c in CLEANING_DROP_COLS if c in df.columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui5jL2fKfzY5"
      },
      "source": [
        "## 4. Feature Engineering and Target Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyN9-ZI_f7pv"
      },
      "source": [
        "### 4.1 Fix data types\n",
        "\n",
        "In this step, I make sure that the core columns used in the model have suitable types and no missing values. The `budget` column is stored as text in the raw data, so I first convert it to a numeric type. Similarly, `release_date` is converted from text to a proper datetime type. Both conversions use `errors=\"coerce\"`, which replaces invalid or unexpected values with missing values instead of raising an error.\n",
        "\n",
        "After these conversions, I drop any rows where `budget`, `revenue`, `release_date`, or `runtime` are missing. These four columns are essential for the project: `revenue` is the target variable, and `budget`, `runtime`, and `release_date` (later used to create `release_year`) are important input features. Working only with rows where these fields are present ensures that the later feature engineering and modeling steps are built on a consistent and reliable subset of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-WuAmP_fzOb"
      },
      "outputs": [],
      "source": [
        "df[\"budget\"] = pd.to_numeric(df[\"budget\"], errors=\"coerce\")\n",
        "df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"budget\", \"revenue\", \"release_date\", \"runtime\"]).copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2hBmmylitx6"
      },
      "source": [
        "### 4.2 Genre multi-hot encoding\n",
        "In this step, I use the genre information as a pre-release feature because we usually know a movie‚Äôs genres before it comes out, and they can strongly influence revenue, for example, we might expect horror movies to have relatively good revenue on average, so genre can be an important signal to look at. The original genres column is a JSON-like string, so I first parse it and extract the list of genre names for each movie. Since a movie can have more than one genre, I apply multi-hot encoding with MultiLabelBinarizer to create one binary column per genre (for example genre_action, genre_comedy, genre_drama), where each column is 1 if the movie belongs to that genre and 0 otherwise. This converts the multi-genre data into a clean numeric format that the model can use, and after that the original genres and genre_list columns are no longer needed and are removed.\n",
        "At the end I demonstrates how a distribution of movies in each genre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7ApE04Uitmb"
      },
      "outputs": [],
      "source": [
        "def extract_genre_names(genres_str):\n",
        "    try:\n",
        "        genres = ast.literal_eval(genres_str)\n",
        "        return [g[\"name\"] for g in genres]\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "df[\"genre_list\"] = df[\"genres\"].apply(extract_genre_names)\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "genre_encoded = mlb.fit_transform(df[\"genre_list\"])\n",
        "\n",
        "genre_df = pd.DataFrame(\n",
        "    genre_encoded,\n",
        "    columns=[f\"genre_{g.lower()}\" for g in mlb.classes_],\n",
        "    index=df.index\n",
        ")\n",
        "\n",
        "df = pd.concat([df, genre_df], axis=1)\n",
        "df = df.drop(columns=[\"genres\", \"genre_list\"])\n",
        "\n",
        "# Find all genre columns\n",
        "genre_cols = [c for c in df.columns if c.startswith(\"genre_\")]\n",
        "\n",
        "# Count how many movies have each genre (1 = present)\n",
        "genre_counts = df[genre_cols].sum().sort_values(ascending=False)\n",
        "\n",
        "genre_counts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hORbj3lcouBR"
      },
      "source": [
        "### 4.3 Cast star-power features (top‚Äë5 actors)\n",
        "\n",
        "In this step, I introduce cast star-power features based on the historical performance of the top actors in each movie.\n",
        "\n",
        "For each movie, only the first five actors listed in the cast are considered (That's just my desicion and it could be more, but for now and the scale of project I picked 5 stars).\n",
        "\n",
        "The dataset lists actors in an ordered way, where more famous actors usually appear first, and the rest follow in this order.\n",
        "\n",
        "To avoid target leakage, actor performance is computed in a time-aware manner. For each actor, only revenues from movies released before the current movie are used. From this history, two statistics are derived:\n",
        "\n",
        "\n",
        "*   the median past revenue, which provides a robust estimate of typical commercial performance, and\n",
        "*  the number of past movies, which captures experience and visibility.\n",
        "\n",
        "I chose the median instead of the mean to reduce bias in the data. Using the mean could be strongly affected by a single extreme case, such as one very unsuccessful movie or one extreme blockbuster in an actor‚Äôs past. In some cases, a movie may succeed because of other leading actors, while the selected actor appears only in a lower billing position (for example, fifth in the cast because I picked first 5 actors). To avoid these misleading effects and obtain a more stable representation of typical performance, I used the median.\n",
        "\n",
        "These actor-level statistics are then aggregated at the movie level across the top-five actors. The median is used for revenue aggregation to reduce the influence of extreme blockbuster outliers, while past movie counts are summed to reflect overall cast experience.\n",
        "\n",
        "The result is a compact set of numerical features that encode cast strength using only information that would have been available prior to a movie‚Äôs release."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5fyK-zwot10"
      },
      "outputs": [],
      "source": [
        "required_cols = [\"id\", \"cast\", \"revenue\", \"release_date\"]\n",
        "missing_required = [c for c in required_cols if c not in df.columns]\n",
        "if missing_required:\n",
        "    raise ValueError(f\"df is missing required columns: {missing_required}\")\n",
        "\n",
        "# here just make sure movie released and not rumored\n",
        "if \"status\" in df.columns:\n",
        "    df = df[df[\"status\"] == \"Released\"].copy()\n",
        "\n",
        "df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"release_date\"]).copy()\n",
        "df[\"release_year\"] = df[\"release_date\"].dt.year.astype(int)\n",
        "\n",
        "\n",
        "df[\"revenue\"] = pd.to_numeric(df[\"revenue\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# extract top 5 actors of each movie\n",
        "def parse_top5_actor_ids(cast_str, k=5):\n",
        "    if pd.isna(cast_str):\n",
        "        return []\n",
        "    try:\n",
        "        cast_list = ast.literal_eval(cast_str)\n",
        "        if not isinstance(cast_list, list):\n",
        "            return []\n",
        "        ids = []\n",
        "        for p in cast_list[:k]:\n",
        "            if isinstance(p, dict) and p.get(\"id\") is not None:\n",
        "                ids.append(int(p[\"id\"]))\n",
        "        return ids\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "df[\"top5_actor_ids\"] = df[\"cast\"].apply(parse_top5_actor_ids)\n",
        "\n",
        "\n",
        "# build actor_movies table (movie x actor rows) from top5 actors\n",
        "actor_movies = df[[\"id\", \"release_year\", \"revenue\", \"top5_actor_ids\"]].explode(\"top5_actor_ids\")\n",
        "actor_movies = actor_movies.dropna(subset=[\"top5_actor_ids\"]).copy()\n",
        "actor_movies = actor_movies.rename(columns={\"top5_actor_ids\": \"actor_id\"})\n",
        "actor_movies[\"actor_id\"] = actor_movies[\"actor_id\"].astype(int)\n",
        "\n",
        "actor_movies_hist = actor_movies[actor_movies[\"revenue\"] > 0].copy()\n",
        "actor_movies_hist = actor_movies_hist.sort_values([\"actor_id\", \"release_year\", \"id\"])\n",
        "\n",
        "# actor past median revenue\n",
        "actor_movies_hist[\"actor_past_median_revenue\"] = (\n",
        "    actor_movies_hist\n",
        "    .groupby(\"actor_id\")[\"revenue\"]\n",
        "    .expanding()\n",
        "    .median()\n",
        "    .shift(1)\n",
        "    .reset_index(level=0, drop=True)\n",
        ")\n",
        "\n",
        "# past movie count for each actor (how many prior movies observed)\n",
        "actor_movies_hist[\"actor_past_movie_count\"] = actor_movies_hist.groupby(\"actor_id\").cumcount()\n",
        "\n",
        "\n",
        "movie_star_power = actor_movies_hist.groupby(\"id\").agg(\n",
        "    top5_cast_median_past_revenue=(\"actor_past_median_revenue\", \"median\"),\n",
        "    top5_cast_total_past_movies=(\"actor_past_movie_count\", \"sum\"),\n",
        ").reset_index()\n",
        "\n",
        "# merge actors table to their movies\n",
        "df = df.merge(movie_star_power, on=\"id\", how=\"left\")\n",
        "\n",
        "df[\"top5_cast_median_past_revenue\"] = df[\"top5_cast_median_past_revenue\"].fillna(0)\n",
        "df[\"top5_cast_total_past_movies\"] = df[\"top5_cast_total_past_movies\"].fillna(0)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrHhY6OiDc3_"
      },
      "source": [
        "### 4.4 Crew track‚Äërecord features (director, writer, producer)\n",
        "\n",
        "In this step, I apply the same approach used in Section 4.3 to key crew roles, specifically the director, writer, and producer. I believe these roles have a strong influence on movie revenue; for example, we would normally expect movies directed by Christopher Nolan to generate higher revenue.\n",
        "\n",
        "For each movie, crew members in these roles are identified, and their historical performance is calculated in a time-aware manner, using only movies released before the current one to avoid target leakage as I discussed in last section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDspaKWbDctV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "DROP_PREV_CREW = [\n",
        "    \"director_past_median_revenue\",\n",
        "    \"director_past_movie_count\",\n",
        "    \"writer_past_median_revenue\",\n",
        "    \"writer_past_movie_count\",\n",
        "    \"producer_past_median_revenue\",\n",
        "    \"producer_past_movie_count\",\n",
        "]\n",
        "df = df.drop(columns=[c for c in DROP_PREV_CREW if c in df.columns], errors=\"ignore\")\n",
        "\n",
        "\n",
        "def parse_crew(crew_str):\n",
        "    if pd.isna(crew_str):\n",
        "        return []\n",
        "    try:\n",
        "        crew = ast.literal_eval(crew_str)\n",
        "        return crew if isinstance(crew, list) else []\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "df[\"crew_parsed\"] = df[\"crew\"].apply(parse_crew)\n",
        "\n",
        "# extract those roles ids\n",
        "def extract_role_ids(crew, roles):\n",
        "    ids = []\n",
        "    for p in crew:\n",
        "        if isinstance(p, dict) and p.get(\"job\") in roles and p.get(\"id\") is not None:\n",
        "            ids.append(int(p[\"id\"]))\n",
        "    return ids\n",
        "\n",
        "DIRECTOR_ROLES = {\"Director\"}\n",
        "WRITER_ROLES = {\"Writer\", \"Screenplay\", \"Story\", \"Original Story\"}\n",
        "PRODUCER_ROLES = {\"Producer\", \"Executive Producer\", \"Co-Producer\", \"Associate Producer\"}\n",
        "\n",
        "df[\"director_ids\"] = df[\"crew_parsed\"].apply(lambda x: extract_role_ids(x, DIRECTOR_ROLES))\n",
        "df[\"writer_ids\"]   = df[\"crew_parsed\"].apply(lambda x: extract_role_ids(x, WRITER_ROLES))\n",
        "df[\"producer_ids\"] = df[\"crew_parsed\"].apply(lambda x: extract_role_ids(x, PRODUCER_ROLES))\n",
        "\n",
        "\n",
        "def build_role_history(df, id_col, role_name):\n",
        "    role_movies = df[[\"id\", \"release_year\", \"revenue\", id_col]].explode(id_col)\n",
        "    role_movies = role_movies.dropna(subset=[id_col]).copy()\n",
        "    role_movies = role_movies.rename(columns={id_col: \"person_id\"})\n",
        "    role_movies[\"person_id\"] = role_movies[\"person_id\"].astype(int)\n",
        "\n",
        "    role_movies = role_movies[role_movies[\"revenue\"] > 0].copy()\n",
        "    role_movies = role_movies.sort_values([\"person_id\", \"release_year\", \"id\"])\n",
        "\n",
        "    role_movies[f\"{role_name}_past_median_revenue\"] = (\n",
        "        role_movies\n",
        "        .groupby(\"person_id\")[\"revenue\"]\n",
        "        .expanding()\n",
        "        .median()\n",
        "        .shift(1)\n",
        "        .reset_index(level=0, drop=True)\n",
        "    )\n",
        "\n",
        "    role_movies[f\"{role_name}_past_movie_count\"] = role_movies.groupby(\"person_id\").cumcount()\n",
        "\n",
        "    agg = role_movies.groupby(\"id\").agg(\n",
        "        **{\n",
        "            f\"{role_name}_past_median_revenue\": (f\"{role_name}_past_median_revenue\", \"median\"),\n",
        "            f\"{role_name}_past_movie_count\": (f\"{role_name}_past_movie_count\", \"sum\"),\n",
        "        }\n",
        "    ).reset_index()\n",
        "\n",
        "    return agg\n",
        "\n",
        "\n",
        "director_agg = build_role_history(df, \"director_ids\", \"director\")\n",
        "writer_agg   = build_role_history(df, \"writer_ids\",   \"writer\")\n",
        "producer_agg = build_role_history(df, \"producer_ids\", \"producer\")\n",
        "\n",
        "# merge tables of each role to the original movie table\n",
        "df = df.merge(director_agg, on=\"id\", how=\"left\")\n",
        "df = df.merge(writer_agg,   on=\"id\", how=\"left\")\n",
        "df = df.merge(producer_agg, on=\"id\", how=\"left\")\n",
        "\n",
        "\n",
        "for col in DROP_PREV_CREW:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].fillna(0)\n",
        "\n",
        "\n",
        "df = df.drop(\n",
        "    columns=[\"crew_parsed\", \"director_ids\", \"writer_ids\", \"producer_ids\"],\n",
        "    errors=\"ignore\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "our1s1Ns0Vpo"
      },
      "source": [
        "## 5. Feature Matrix Preparation\n",
        "\n",
        "In this section I build the final modeling dataset (**df_model**), define the target label (**y**), and construct the feature matrix (**X**).\n",
        "At this point, all cleaning and feature engineering steps are complete, so the remaining work is about turning the table into a purely numerical format that our model can learn from.\n",
        "\n",
        "**What happens in this section**\n",
        "- Select / drop non-model columns (IDs, raw JSON strings, and high-cardinality text fields)\n",
        "- Define the target as **log1p(revenue)** to reduce skewness\n",
        "- One-hot encode remaining low-cardinality categorical features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Target distribution and motivation for log transform\n",
        "\n",
        "From practice and common sense, movie revenue is usually not evenly distributed. Most movies earn a small or medium amount, and only a few become very successful blockbusters. This means the revenue values are likely to be **strongly right-skewed**.\n",
        "\n",
        "To confirm this in the current dataset, I first look at basic statistics, the skewness value, and a histogram of the raw revenue (for movies with positive revenue). After that, I apply a `log(1 + revenue)` transform and check the new distribution again.\n",
        "\n",
        "The log transform compresses very large values much more than small ones and makes the distribution more balanced. Based on this behaviour in the plots and skewness values, I decide to use `log(1 + revenue)` as the target (`log_revenue`) for the regression model, because it helps the model learn more stable patterns and reduces the influence of extreme outliers.\n"
      ],
      "metadata": {
        "id": "ts2bdC-dfHzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rev = df[\"revenue\"]\n",
        "rev_pos = rev[rev > 0]\n",
        "\n",
        "print(\"\\nSkewness of raw revenue:\", rev_pos.skew())\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(rev_pos / 1e6, bins=100)\n",
        "plt.xlabel(\"Revenue (millions of USD)\")\n",
        "plt.ylabel(\"Number of movies\")\n",
        "plt.title(\"Distribution of movie revenue (raw, > 0)\")\n",
        "plt.show()\n",
        "\n",
        "rev_log = np.log1p(rev_pos)\n",
        "\n",
        "print(\"\\nSkewness of log(1 + revenue):\", rev_log.skew())\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(rev_log, bins=50)\n",
        "plt.xlabel(\"log(1 + revenue)\")\n",
        "plt.ylabel(\"Number of movies\")\n",
        "plt.title(\"Distribution of log-transformed movie revenue\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vDp3c_Yjgjpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZrGAQZ7BFTW"
      },
      "source": [
        "### 5.2 Final Feature Matrix and Target Construction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWjOJXg292_3"
      },
      "outputs": [],
      "source": [
        "df_model = df.copy()\n",
        "df_model = df_model[df_model[\"revenue\"] > 0].copy()\n",
        "\n",
        "# target: log-transformed revenue\n",
        "df_model[\"log_revenue\"] = np.log1p(df_model[\"revenue\"])\n",
        "y = df_model[\"log_revenue\"]\n",
        "\n",
        "DROP_COLS = [\n",
        "    # target\n",
        "    \"revenue\", \"log_revenue\",\n",
        "\n",
        "    # identifiers / raw text\n",
        "    \"id\", \"title\", \"release_date\",\n",
        "    \"cast\", \"crew\",\n",
        "\n",
        "    # high-cardinality categoricals (kept simple for this course project)\n",
        "    \"production_companies\", \"production_countries\", \"spoken_languages\",\n",
        "\n",
        "    # raw JSON column (genres already expanded into multi-hot columns earlier)\n",
        "    \"genres\",\n",
        "]\n",
        "\n",
        "X = df_model.drop(columns=[c for c in DROP_COLS if c in df_model.columns], errors=\"ignore\").copy()\n",
        "\n",
        "\n",
        "bad_cols = [\n",
        "    c for c in X.columns\n",
        "    if X[c].apply(lambda v: isinstance(v, (list, dict, set, tuple))).any()\n",
        "]\n",
        "X = X.drop(columns=bad_cols, errors=\"ignore\")\n",
        "print(\"Dropped non-tabular columns:\", bad_cols)\n",
        "\n",
        "# handle missing numeric values\n",
        "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "X[num_cols] = X[num_cols].fillna(0)\n",
        "\n",
        "# One-hot encode remaining categorical features (low-cardinality)\n",
        "cat_cols = X.select_dtypes(include=[\"object\"]).columns\n",
        "print(\"Categorical columns encoded:\", cat_cols.tolist())\n",
        "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
        "\n",
        "print(\"‚úÖ Final shapes -> X:\", X.shape, \"| y:\", y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sP9NUHfM0KK"
      },
      "source": [
        "## 6. Split Train and Test\n",
        "In this step, the dataset is split into training and test sets using a time-based split rather than a random split.\n",
        "\n",
        "Movies released before 2015 are used for training, while movies released in 2015 and later are kept for testing. This choice reflects a realistic prediction scenario, where a model is trained on historical data and then used to estimate the revenue of future movies.\n",
        "\n",
        "Using a time-based split helps prevent data leakage, since information from newer movies is not allowed to influence the training process. It also better matches the real-world use case of this project, where revenue predictions are made for movies that have not yet been released.\n",
        "\n",
        "I did not tune the split year to optimize performance. The goal was to simulate a realistic forecasting scenario with a clear time boundary, and 2015 provided a good balance between historical coverage and test set size (I showed it below and the test size is 9.55% of the full dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7X2EfFQ92_4"
      },
      "outputs": [],
      "source": [
        "train_idx = df_model[\"release_year\"] < 2015\n",
        "test_idx  = df_model[\"release_year\"] >= 2015\n",
        "\n",
        "total_samples = len(df_model)\n",
        "test_samples = test_idx.sum()\n",
        "\n",
        "test_percentage = (test_samples / total_samples) * 100\n",
        "\n",
        "print(\"Test percentage:\", test_percentage)\n",
        "\n",
        "X_train = X.loc[train_idx].copy()\n",
        "X_test  = X.loc[test_idx].copy()\n",
        "y_train = y.loc[train_idx].copy()\n",
        "y_test  = y.loc[test_idx].copy()\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMw0cymZ92_4"
      },
      "source": [
        "## 7. Model selection and evaluation\n",
        "\n",
        "In this section, I compare a small set of models using cross-validation only on the training period, then evaluate the best model once on the held-out test period.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO59YsPeNQQT"
      },
      "source": [
        "### 7.1 Model selection with cross‚Äëvalidation (GridSearchCV)\n",
        "Model Selection Rationale\n",
        "\n",
        "Here multiple regression models are evaluated to understand how different modeling assumptions affect revenue prediction. Each model is chosen for a specific reason, ranging from simple baselines to more expressive non-linear methods.\n",
        "\n",
        "Ridge Regression is used as a linear baseline model. It extends standard linear regression by adding regularization, which helps control overfitting when the number of features is large. Ridge regression also provides a strong reference point to compare more complex models against a simple, interpretable approach.\n",
        "\n",
        "Random Forest Regressor is included to capture non-linear relationships and feature interactions that linear models cannot represent. By combining many decision trees, random forests are robust to noise and can model complex patterns in tabular data without requiring strong assumptions about feature distributions.\n",
        "\n",
        "Histogram-based Gradient Boosting Regressor is selected as a more advanced ensemble method that incrementally improves predictions by correcting previous errors. This model is well-suited for large tabular datasets, handles non-linearities efficiently, and often achieves strong performance with relatively little feature scaling or manual tuning.\n",
        "\n",
        "Together, these models provide a balanced comparison between interpretability, flexibility, and predictive performance, allowing the final model choice to be based on empirical results rather than assumptions.\n",
        "\n",
        "And it's good to mention that the Tree-based models (Random Forest, Gradient Boosting) do not require feature scaling.\n",
        "For linear models (here Ridge), scaling is applied inside the sklearn Pipeline during model selection to avoid data leakage.\n",
        "\n",
        "And for validation I went with K-Fold cross-validation as a balanced and practical approach. A single hold-out split can lead to unstable results depending on how the data is divided, while K-Fold reduces this risk by averaging performance across multiple folds. Leave-one-out validation is also a computationally expensive method for a dataset of this size and does not provide clear advantages here. Although the data has a time component, the temporal dependency is already handled by the train‚Äìtest split, and K-Fold cross-validation within the training set offers a good trade-off between reliability and efficiency for model selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j62gdGGf92_5"
      },
      "outputs": [],
      "source": [
        "\n",
        "models = []\n",
        "\n",
        "# ridge\n",
        "pipe_ridge = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"ridge\", Ridge(random_state=42))\n",
        "])\n",
        "param_grid_ridge = {\"ridge__alpha\": [0.1, 1.0, 10.0, 50.0]}\n",
        "models.append((\"Ridge\", pipe_ridge, param_grid_ridge))\n",
        "\n",
        "# random forest\n",
        "pipe_rf = Pipeline([\n",
        "    (\"rf\", RandomForestRegressor(random_state=42, n_jobs=-1))\n",
        "])\n",
        "param_grid_rf = {\n",
        "    \"rf__n_estimators\": [200, 400],\n",
        "    \"rf__max_depth\": [10, 15, 20],\n",
        "    \"rf__min_samples_leaf\": [5, 10, 20],\n",
        "    \"rf__max_features\": [\"sqrt\", 0.5],\n",
        "}\n",
        "models.append((\"Random Forest\", pipe_rf, param_grid_rf))\n",
        "\n",
        "# histGradientBoosting\n",
        "pipe_hgb = Pipeline([\n",
        "    (\"hgb\", HistGradientBoostingRegressor(random_state=42))\n",
        "])\n",
        "param_grid_hgb = {\n",
        "    \"hgb__max_depth\": [4, 6, 8],\n",
        "    \"hgb__learning_rate\": [0.03, 0.05, 0.08],\n",
        "    \"hgb__max_iter\": [200, 300, 500],\n",
        "}\n",
        "models.append((\"HistGradientBoosting\", pipe_hgb, param_grid_hgb))\n",
        "\n",
        "\n",
        "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "def count_grid_fits(param_grid, cv):\n",
        "    n = 1\n",
        "    for v in param_grid.values():\n",
        "        n *= len(v)\n",
        "    return n * cv.get_n_splits()\n",
        "\n",
        "selection_rows = []\n",
        "best_estimators = {}\n",
        "\n",
        "for name, pipe, param_grid in models:\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "\n",
        "    grid = GridSearchCV(\n",
        "        estimator=pipe,\n",
        "        param_grid=param_grid,\n",
        "        cv=cv,\n",
        "        scoring=\"neg_mean_absolute_error\",\n",
        "        n_jobs=-1,\n",
        "        refit=True,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    total = count_grid_fits(param_grid, cv)\n",
        "    with tqdm_joblib(tqdm(total=total, desc=f\"{name} fits\")):\n",
        "        grid.fit(X_train, y_train)\n",
        "\n",
        "    best_cv_mae = -grid.best_score_\n",
        "    best_estimators[name] = grid.best_estimator_\n",
        "\n",
        "    y_pred = grid.best_estimator_.predict(X_test)\n",
        "    test_mae = mean_absolute_error(y_test, y_pred)\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "    selection_rows.append({\n",
        "        \"model\": name,\n",
        "        \"best_params\": grid.best_params_,\n",
        "        \"cv_mae_log\": best_cv_mae,\n",
        "        \"test_mae_log\": test_mae,\n",
        "        \"test_rmse_log\": test_rmse,\n",
        "        \"typical_mult_error\": float(np.exp(test_mae)),\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(selection_rows).sort_values(\"test_mae_log\")\n",
        "display(results_df)\n",
        "\n",
        "best_name = results_df.iloc[0][\"model\"]\n",
        "best_estimator = best_estimators[best_name]\n",
        "\n",
        "print(\"\\n‚úÖ Selected best model:\", best_name,best_estimator)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PF_VF0eaTo8"
      },
      "source": [
        "## 8. Final model\n",
        "In this section, based on the earlier model comparison, I use the HistGradientBoostingRegressor with the best hyperparameters found in cross-validation, fit it on the training set (X_train, y_train), and then predict the log-revenue for the test set (X_test). From these predictions, I calculate the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) in log space to measure how far the predictions are from the true values. Finally, I convert the MAE in log space into a multiplicative error factor using the exponential function, which shows how much the model‚Äôs predictions typically differ from the true revenue in relative terms and makes the results easier to interpret in the original scale.\n",
        "\n",
        "In the next section, I will discuss the rationale behind these decisions in more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eesUbAGaTXL"
      },
      "outputs": [],
      "source": [
        "\n",
        "final_model = best_estimator\n",
        "\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_test_pred = final_model.predict(X_test)\n",
        "\n",
        "final_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "final_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "print(\"üéØ FINAL MODEL PERFORMANCE\")\n",
        "print(f\"MAE (log revenue):  {final_mae:.4f}\")\n",
        "print(f\"RMSE (log revenue): {final_rmse:.4f}\")\n",
        "\n",
        "mult_error = np.exp(final_mae)\n",
        "print(f\"Typical multiplicative error (On average, the model‚Äôs prediction is off by a factor of about {mult_error:.2f} compared to the true revenue.): √∑{mult_error:.2f} to √ó{mult_error:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1KY6rX492_5"
      },
      "source": [
        "### 8.1 Interpreting predictions as a revenue range\n",
        "\n",
        "These values are computed on the log-transformed target log(1 + revenue).\n",
        "An MAE of 1.3914 in log space means that, on average, the absolute difference between the predicted and true log(1 + revenue) is about 1.39. When we map this back to the original revenue scale, this corresponds to a multiplicative error of about exp(1.3914) ‚âà 4.02. In practical terms, a typical prediction is within a factor of about 4 of the true revenue. For example, if the model predicts 10 million dollars for a movie, the true revenue is often somewhere in a range roughly between 10 / 4 ‚âà 2.5 million and 10 √ó 4 ‚âà 40 million.\n",
        "\n",
        "The RMSE of 1.9353 in log space is larger than the MAE, which is expected, because RMSE gives more weight to larger errors. It mainly confirms that there are some movies where the model makes bigger mistakes, but for the typical case the multiplicative error factor ‚âà 4.02 (derived from MAE) is the main quantity I use to interpret and present the model‚Äôs performance.\n",
        "\n",
        "\n",
        "**Evaluation metrics**\n",
        "\n",
        "I did not use accuracy, because it is a metric for classification, not regression. Accuracy measures how often the predicted class label is correct (for example ‚Äúhit vs. flop‚Äù). In this project, the target is a continuous log-revenue value, not a class label, so accuracy is not meaningful here. There is no ‚Äúcorrect‚Äù or ‚Äúincorrect‚Äù class, only predictions that are closer or farther from the true value.\n",
        "\n",
        "I use **MAE** as the main evaluation metric, because it is more robust to extreme values in this skewed, log-transformed target and directly measures the typical absolute prediction error. I still compute **RMSE** as a secondary metric to report a squared-error perspective, but I do not use it for model selection or for building the revenue range, since RMSE is highly sensitive to a few very large residuals. I also do not rely on **R¬≤** as the primary metric, because it focuses on the proportion of variance explained and is scale-free."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Discussion and limitations\n",
        "\n",
        "- **Data quality and coverage.** The raw dataset has a major limitation: around 84% of movies have a recorded revenue of zero, which in many cases likely means that the true revenue is missing or not reported. For this reason, the modeling dataset is restricted to movies with strictly positive revenue. This improves target quality but reduces the effective sample size and may bias the model towards better-documented and more commercially visible titles. In addition, some core features such as `budget` and `runtime` also contain missing or noisy values and require filtering, which further reduces the dataset.(I shown this in 3.6.0 section)\n",
        "\n",
        "- **Limited pre-release information.** The model is intentionally restricted to features that are available before release (budget, genre, release year, cast and crew history). However, several strong drivers of revenue are not included, such as marketing spend, distribution strategy, release window, competition from other releases, and brand/franchise effects. As a result, the model can only capture part of the true signal, and its predictions should be seen as coarse forecasts rather than precise financial estimates.\n",
        "\n",
        "- **Approximate modeling of star power and track record.** The cast and crew features rely on approximations: actors are selected based on billing order (top 5 cast members), and historical performance for directors, writers, and producers is summarized via median past revenue and past movie counts. These features reduce sparsity and capture some notion of ‚Äútrack record‚Äù, but they cannot fully represent how audience perception, role importance, or uncredited work influence revenue in the real world.\n",
        "\n",
        "- **Model accuracy and uncertainty.** Even after log transformation, the final model‚Äôs typical multiplicative error is around a factor of 4 on the original revenue scale. This means that predictions are often within a broad revenue range rather than close to the exact value. For this reason, model outputs are interpreted as revenue intervals based on the typical error, instead of precise point estimates."
      ],
      "metadata": {
        "id": "lH9MVW8T3oWv"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}